# DemoAPI ‚Äì API Automation Testing Framework

---

## üìå Overview

üîç **What This Project Really Demonstrates**

- **Framework-oriented testing mindset**  
  Instead of writing scattered API test scripts, this project focuses on structuring API tests into a reusable and maintainable framework.

- **Clear separation of concerns**  
  Test logic, test data, and execution flow are separated, allowing tests to scale as the project grows.

- **Predictable and maintainable API regression testing**  
  By introducing Excel-based data-driven testing, standardized request handling, and unified HTML reporting, the framework makes regression testing easier to maintain and easier to understand‚Äîespecially when test cases change frequently.

üí° **When This Kind of Framework Is Useful**

This approach is especially suitable for:

- **Small to mid-sized backend systems**
- **Teams that rely heavily on API regression testing**
- **Scenarios where test cases change frequently and require clear traceability**


The framework is built using:

- **Python 3**
- **unittest**
- **DDT (Data-Driven Testing)**
- **Excel** for test case & test data management
- **HTMLTestRunner** for automated HTML report generation

---

## ‚ùì Why Build a Custom API Testing Framework?

Although there are many open-source API testing tools such as **Postman** and **JMeter**, they have several practical limitations in real projects.

### Key Limitations of Existing Tools

- **Test data is not controllable**  
  When API response data changes dynamically, it becomes difficult to perform stable assertions.  
  Failures may be caused by **code defects** or by **test data changes**, which are hard to distinguish.  
  Therefore, **test data initialization** and controlled test data are required.

- **Limited extensibility**  
  Many open-source tools are difficult to extend.  
  Customizing **test report formats** or integrating API tests into **CI pipelines** for scheduled execution is often inconvenient or unsupported.

---

## üîÅ API Testing Framework Workflow

Below is a sample HTML test report generated by this framework:

<img src="share/screenshots/Summary.png" width="800">

### Overall Workflow

1. Call the **target system APIs** using **data-driven testing**, reading test cases line by line from Excel.
2. Send API requests and compare API responses with expected values defined in Excel.
3. Use the **unittest** framework to assert API responses and generate **HTML test reports**.

---

## üìÇ Project Directory Structure

| Directory / File | Description |
|------------------|-------------|
| `config/` | Path and environment configuration files |
| `database/` | Test case templates and configuration files |
| `lib/` | Core framework modules (Excel read/write, request handling) |
| `package/` | Third-party libraries (HTMLTestRunner) |
| `report/` | Generated API automation test reports |
| `testcase/` | API automation test cases |
| `run_demo.py` | Main entry script to execute all API test cases |

---

## ‚ñ∂Ô∏è How to Run This Project

> **Note:** This is a public demonstration project.  
> Environment-specific configurations (database credentials, email accounts) are intentionally excluded.

### 1. Environment Setup

- Python 3.x
- Required Python packages:
  - requests
  - unittest
  - ddt
  - openpyxl

### 2. Configuration

Create a local configuration file based on the example template:

```bash
cp database/config.ini.example database/config.ini
```

Fill in your local database and email configuration if you want to execute the project end-to-end.

### 3. Execute Tests

Run the main entry script:

```bash
python run_demo.py
```

After execution, an HTML test report will be generated under the `report/` directory.

---

## ‚úÖ Notes for Reviewers

- This repository focuses on **framework design and structure**, not environment-specific setup.
- Sensitive configuration files are excluded for security reasons.
- The project is designed to be **readable, extensible, and CI-friendly**.
